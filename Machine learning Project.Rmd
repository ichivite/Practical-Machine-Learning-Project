---
title: "Practical Machine Learning: Prediction Assignment Writeup"
author: "I. Chivite"
date: "October/2017"
output: html_document
---

```{r preliminaries, include=FALSE, cache=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE)
set.seed(1)
```



## Introduction 

Participants were asked to perform one set of 10 repetitions of the *Unilateral Dumbbell Biceps Curl* in five different ways: 

* **Class A:** Exactly according to the specification
* **Class B:** Throwing the elbows to the front
* **Class C:** Lifting the dumbbell only halfway 
* **Class D:** Lowering the dumbbell only halfway
* **Class E:** Throwing the hips to the front 

Class A is the right way, while the other 4 classes correspond to common mistakes. A full description of the study and methodology is available at [*Velloso, et al*](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201)

## Feature extraction and selection

![Sensor Configuration. Figure from [*Velloso, et al*](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201)](werables.jpeg)

* Readings from sensors on the belt, forearm, arm, and dumbbell were collected on each of the six subjects in overlapping time windows of 2.5 s.   

* The Euler angles (roll, pitch and yaw) were specified together with other derived features.    

* Raw accelerometer, gyroscope and magnetometer readings were also recorded.  

* For the Euler angles of each of the four sensors, derived features were obtained. They are: mean, variance, standard deviation, max and min.   

* The set of criteria for the choice of these features and derived quantities, a total seventeen variables, is described in the original publication <sup>[1](#myfootnote1)</sup>.   

#### Features selected: (mean, var, max and min per window)  

**Belt**

|    |   Feature   |   Column Name   |   Associated variables   |   
|----:|------------:|---------------:|--------------------------:|
|1 | mean roll | avg_roll_belt|   roll_belt   |    
|2 | variance of the roll  |   var_roll_belt  |   roll_belt   |   
|3 |  maximum of the accelerometer vector|    |accel_belt_x,  accel_belt_y,  accel_belt_z|     
|4   |  range of the accelerometer vector |   |accel_belt_x,  accel_belt_y,  accel_belt_z|     
|5 |   variance of the accelerometer vector|   |   accel_belt_x,  accel_belt_y,  accel_belt_z|   
|6  |   variance of the gyro|   |   gyros_belt_x,  gyros_belt_y, gyros_belt_z   |   
|7  |   variance of the magnetometer|   | magnet_belt_x,	magnet_belt_y  |   
  

**Arm**   

|    |   Feature   |   Column Name   |   Associated variables   |   
|----:|------------:|---------------:|--------------------------:|
|8 | variance of the accelerometer vector: | var_accel_arm| accel_arm_x,	accel_arm_y,	accel_arm_z     |    
|9 | maximum of the magnetometer |     |  magnet_arm_x,	magnet_arm_y,	magnet_arm_z   |
|10 |   minimum of the magnetometer|    |magnet_arm_x,	magnet_arm_y,	magnet_arm_z|     


**Dumbbell**

|    |   Feature   |   Column Name   |   Associated variables   |   
|----:|------------:|---------------:|--------------------------:|
|11 | maximum of the acceleration | total_accel_dumbbell |   accel_dumbbell_x, accel_dumbbell_y, accel_dumbbell_z   |    
|12 | variance of the gyro  |    |   gyros_dumbbell_x,	gyros_dumbbell_y,	gyros_dumbbell_z   |   
|13 |  maximum of magnetometer|    |magnet_dumbbell_x,	magnet_dumbbell_y,	magnet_dumbbell_z|     
|14   |  minimum of the magnetometer |   |magnet_dumbbell_x	magnet_dumbbell_y	magnet_dumbbell_z|    

**Glove** 


|    |   Feature   |   Column Name   |   Associated variables   |   
|----:|------------:|---------------:|--------------------------:|
|15 | sum of the pitch |  |   pitch_forearm, avg_pitch_forearm   |    
|16 | maximum of the gyro  |    |   gyros_forearm_x,	gyros_forearm_y,	gyros_forearm_z   |   
|17 |  minimum of the gyro|    |gyros_forearm_x,	gyros_forearm_y,	gyros_forearm_z|   

The derived features were calculated using the Euclidean magnitude of the associated vector:

$$
F=\left<\sqrt{F_x^2+F_y^2+F_z^2}\right>
$$

Where the average is taken over the corresponding time window (num_window). After this, the max, min, var or just the magnitude are taken.  


## The goal of the project
-----------------------

The description of the assignment contains the following information on the dataset:

> In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

The goal is 

> to predict the manner in which they did the exercise.



# Processing of the Data

Load the necessary libraries:

```{r load libraries, cache=TRUE}
library(caret)
```

Set the working direcotry:
```{r load libraries, cache=TRUE}
setwd("C:/Users/IÑIGO/Documents/coursera/Practical Machine Learning")
```

Read the raw data:

```{r read the training data}
pml_training <- read.csv("pml-training.csv")
pml_testing <- read.csv("pml-testing.csv")
#names(pml_training)
#namees(pml_testing)
```

Remove the columns all value is na in the testing set.

```{r subset for relevant features, cache=TRUE}
training <- pml_training[,-which(apply(pml_testing,2,function(x)all(is.na(x))))]
testing <- pml_testing[,-which(apply(pml_testing,2,function(x)all(is.na(x))))]
```

Remove the datas which new_window is true, because all new_window is no in testing set.

```{r load libraries, cache=TRUE}
training <- training[-which(training$new_window=='yes'),]

```


Remove the columns which no relation with classe.
removed columns : X, user_name, raw_timestamp_part_1, raw_timestamp_part2, cvtd_timestamp, num_window, new_window  

```{r subset for relevant features, cache=TRUE}
training <- training[, -c(1:7)]
testing  <- testing[, -c(1:7)]

```


### Train a model

I used the *random-forest* technique to generate a predictive model. In sum, 10 models were trained. I played around with the parameters passed to `trControl` and specified different models with bootstrapping (`method = "boot"`) and cross-validation (`method = "cv"`).

It took more than one day to train all models. Afterwards I tested their performance on the cross-validation dataset. It turned out that all models showed a good performance (because their accuracy was above 99%) though their training times were quite different.

Due to the similar performance, I will present the model with the shortest training time.


```{r load randomForest package, message=FALSE}
library(randomForest)
```

```{r subset for relevant features, cache=TRUE}
set.seed(1234)

modelFit <- train(classe~., data=training, ntree=150, preProcess=c("center","scale"), method="rf", trControl=trainControl(method="cv"))
```



```{r subset for relevant features, cache=TRUE}
print(modelFit)

plot(modelFit$finalModel, main="Random Forest Error Rates")

```


### Evaluate the model (out-of-sample error)

First, the final model is used to predict the outcome in the dataset.
```{r subset for relevant features, cache=TRUE}

pred <-predict(modelFit, testing)

pred

```




### Variable importance

The five most important variables in the model and their relative importance values are:

```{r varImp}
vi <- varImp(modFit)$importance
vi[head(order(unlist(vi), decreasing = TRUE), 5L), , drop = FALSE]
```

